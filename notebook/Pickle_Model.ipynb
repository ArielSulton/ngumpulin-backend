{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBGcY5MJeuYE",
        "outputId": "faa6b363-bd52-432b-ab5c-da21a47a4d21"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy pandas pygments matplotlib matplotlib-inline seaborn torch gliner supabase scikit-learn scipy yellowbrick hdbscan optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import optuna\n",
        "\n",
        "import torch\n",
        "from gliner import GLiNER\n",
        "from supabase import create_client, Client\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans, BisectingKMeans, DBSCAN, OPTICS, MeanShift, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from hdbscan import HDBSCAN\n",
        "import hdbscan.prediction\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NER Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = GLiNER.from_pretrained(\"gliner-community/gliner_medium-v2.5\")\n",
        "\n",
        "# model.save_pretrained(\"gliner_Med\")\n",
        "# loaded_model = GLiNER.from_pretrained(\"gliner_Med\", load_tokenizer = True, local_files_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = \"\"\"\n",
        "# Libretto by Marius Petipa, based on the 1822 novella ``Trilby, ou Le Lutin d'Argail`` by Charles Nodier, first presented by the Ballet of the Moscow Imperial Bolshoi Theatre on January 25/February 6 (Julian/Gregorian calendar dates), 1870, in Moscow with Polina Karpakova as Trilby and Ludiia Geiten as Miranda and restaged by Petipa for the Imperial Ballet at the Imperial Bolshoi Kamenny Theatre on January 17–29, 1871 in St. Petersburg with Adèle Grantzow as Trilby and Lev Ivanov as Count Leopold.\n",
        "# \"\"\"\n",
        "\n",
        "# labels = [\"person\", \"book\", \"location\", \"date\", \"actor\", \"character\"]\n",
        "\n",
        "# entities = loaded_model.predict_entities(text, labels, threshold=0.4)\n",
        "\n",
        "# for entity in entities:\n",
        "#     print(entity[\"text\"], \"=>\", entity[\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pinecone import Pinecone\n",
        "# from langchain_pinecone import PineconeEmbeddings\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_community.document_loaders import PyPDFLoader\n",
        "# from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "url: str = \"https://alwocqtpmrlfebnjjtct.supabase.co\"\n",
        "key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImFsd29jcXRwbXJsZmVibmpqdGN0Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzQ0NTAzMDIsImV4cCI6MjA1MDAyNjMwMn0._NZ3uFepvW-JplnMj8jRhbf5CoT4QMS6lB5OJQaxFu4\"\n",
        "supabase: Client = create_client(url, key)\n",
        "\n",
        "table_name = \"documents\"\n",
        "response = supabase.table(table_name).select(\"*\").execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(response.data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"DF Shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Descriptive Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "num_cols = df.select_dtypes(include='number').drop(columns=['target'], errors='ignore').columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df[num_cols].describe().T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df = df.dropna(subset=['NRP'])\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_cols = len(num_cols)\n",
        "\n",
        "n_rows = (n_cols + 1) // 2\n",
        "fig, axes = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(num_cols):\n",
        "    ax = axes[i]\n",
        "    sns.kdeplot(df[col], ax=ax, fill=True, color='orange')\n",
        "    ax.set_title(f\"Distribusi Numerik: {col}\")\n",
        "    ax.legend()\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Type Convert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZWdsGnwCfTc3"
      },
      "outputs": [],
      "source": [
        "df['deadline'] = pd.to_datetime(df['deadline'])\n",
        "df['uploadedDate'] = pd.to_datetime(df['uploadedDate'])\n",
        "\n",
        "df['timing'] = (df['deadline'] - df['uploadedDate']).dt.total_seconds() / 3600\n",
        "df['timing'] = df['timing'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plagiarism Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PZBtyXzprS41"
      },
      "outputs": [],
      "source": [
        "plagiarism_rule = [40, 50, 60]\n",
        "\n",
        "no_plagiarism = plagiarism_rule[0]\n",
        "maybe_plagiarism = plagiarism_rule[1]\n",
        "plagiarim = plagiarism_rule[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "il8Vm0LdxBUh"
      },
      "outputs": [],
      "source": [
        "df['plagiarism'] = df['plagiarism'].apply(\n",
        "    lambda row: round(max([v for item in row for v in item.values()]) * 100, 2) if row else 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IlzvObnzyJiE",
        "outputId": "18529b0b-5f4d-441a-c0d1-7bcd750f0c91"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kvKOPiPUe_xf",
        "outputId": "1aea8eb4-1c76-484b-f958-f723f3ad4b20"
      },
      "outputs": [],
      "source": [
        "data=df[['sentences', 'page', 'timing', 'plagiarism']]\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_cols = len(data.columns.tolist())\n",
        "\n",
        "n_rows = (n_cols + 1) // 2\n",
        "fig, axes = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(data.columns.tolist()):\n",
        "    ax = axes[i]\n",
        "    sns.kdeplot(data[col], ax=ax, fill=True)\n",
        "    ax.set_title(f\"Distribusi Numerik: {col}\")\n",
        "    ax.legend()\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Scalling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = data.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(data)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=features)\n",
        "\n",
        "print(X_scaled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# weights = np.array([0.2, 0.3, 2.0, 2.5])     # 0.68 (10 cluster)\n",
        "# weights = np.array([0.15, 0.25, 2.0, 2.77])  # 0.71 (10 cluster) - 0.44 (3 cluster)\n",
        "# weights = np.array([0.1, 0.15, 1.0, 5.0])    # 0.76 (3 cluster)\n",
        "# weights = np.array([0.3, 0.3, 1.5, 5.0])     # 0.67 (3 cluster)\n",
        "weights = np.array([0.5, 0.5, 1.5, 4.5])       # 0.62 (3 cluster)\n",
        "# weights = np.array([0.3, 0.4, 2.0, 4.0])     # 0.52 (3 cluster)\n",
        "# weights = np.array([0.5, 0.7, 2.5, 3.5])     # 0.43 (2 cluster)\n",
        "# weights = np.array([1.0, 1.2, 3.0, 3.5])     # 0.39 (3 cluster)\n",
        "\n",
        "X_weight = X_scaled * weights\n",
        "X_weight = pd.DataFrame(X_weight, columns=features)\n",
        "\n",
        "print(X_weight.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_cols = len(features)\n",
        "\n",
        "n_rows = (n_cols + 1) // 2\n",
        "fig, axes = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(features):\n",
        "    ax = axes[i]\n",
        "    sns.kdeplot(X_weight[col], ax=ax, fill=True)\n",
        "    ax.set_title(f\"Distribusi Numerik: {col}\")\n",
        "    ax.legend()\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(X_weight.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimal Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explore_optimal_clusters(X_weight, max_clusters=10):\n",
        "    # K-means elbow method\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    \n",
        "    plt.subplot(1, 3, 1)\n",
        "    visualizer = KElbowVisualizer(KMeans(random_state=42), k=(2, min(max_clusters, X_weight.shape[0]-1)))\n",
        "    visualizer.fit(X_weight)\n",
        "    visualizer.finalize()\n",
        "    \n",
        "    # Silhouette analysis\n",
        "    plt.subplot(1, 3, 2)\n",
        "    silhouette_scores = []\n",
        "    for k in range(2, min(max_clusters, X_weight.shape[0]-1)+1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X_weight)\n",
        "        try:\n",
        "            score = silhouette_score(X_weight, labels)\n",
        "            silhouette_scores.append(score)\n",
        "        except:\n",
        "            silhouette_scores.append(0)\n",
        "    \n",
        "    plt.plot(range(2, min(max_clusters, X_weight.shape[0]-1)+1), silhouette_scores, marker='o')\n",
        "    plt.title('Silhouette Score vs Number of Clusters')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    \n",
        "    # Hierarchical clustering dendrogram\n",
        "    plt.subplot(1, 3, 3)\n",
        "    \n",
        "    Z = linkage(X_weight, 'ward')\n",
        "    dendrogram(Z)\n",
        "    plt.title('Hierarchical Clustering Dendrogram')\n",
        "    plt.xlabel('Sample index')\n",
        "    plt.ylabel('Distance')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    if len(silhouette_scores) > 0:\n",
        "        optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
        "        return optimal_k\n",
        "    return 3  # Default if analysis fails\n",
        "\n",
        "suggested_clusters = 3\n",
        "# suggested_clusters = explore_optimal_clusters(X_weight, max_clusters=100)\n",
        "print(f\"Suggested optimal number of clusters based on analysis: {suggested_clusters}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optuna Objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_kmeans(trial):\n",
        "    n_clusters = trial.suggest_int('n_clusters', 2, 3)\n",
        "    # n_clusters = trial.suggest_int('n_clusters', 2, 10)\n",
        "    init_method = trial.suggest_categorical('init', ['k-means++', 'random'])\n",
        "    n_init = trial.suggest_int('n_init', 1, 10)\n",
        "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
        "    algorithm = trial.suggest_categorical('algorithm', ['auto', 'full', 'elkan'])\n",
        "    random_state = trial.suggest_int('random_state', 0, 1000)\n",
        "    \n",
        "    model = KMeans(\n",
        "        n_clusters=n_clusters, \n",
        "        init=init_method, \n",
        "        n_init=n_init,\n",
        "        max_iter=max_iter,\n",
        "        algorithm=algorithm,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        if len(set(labels)) <= 1:  # Check if all samples in same cluster\n",
        "            return -1.0\n",
        "        score = silhouette_score(X_weight, labels)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in KMeans: {e}\")\n",
        "        return -1.0\n",
        "\n",
        "def objective_bisecting_kmeans(trial):\n",
        "    n_clusters = trial.suggest_int('n_clusters', 2, 3)\n",
        "    # n_clusters = trial.suggest_int('n_clusters', 2, 10)\n",
        "    init = trial.suggest_categorical('init', ['k-means++', 'random'])\n",
        "    n_init = trial.suggest_int('n_init', 1, 10)\n",
        "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
        "    random_state = trial.suggest_int('random_state', 0, 1000)\n",
        "\n",
        "    model = BisectingKMeans(\n",
        "        n_clusters=n_clusters,\n",
        "        init=init,\n",
        "        n_init=n_init,\n",
        "        max_iter=max_iter,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        if len(set(labels)) <= 1:\n",
        "            return -1.0\n",
        "        score = silhouette_score(X_weight, labels)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in BisectingKMeans: {e}\")\n",
        "        return -1.0\n",
        "\n",
        "\n",
        "def objective_gmm(trial):\n",
        "    n_components = trial.suggest_int('n_components', 2, 3)\n",
        "    # n_components = trial.suggest_int('n_components', 2, 10)\n",
        "    covariance_type = trial.suggest_categorical('covariance_type', ['full', 'tied', 'diag', 'spherical'])\n",
        "    init_params = trial.suggest_categorical('init_params', ['kmeans', 'random'])\n",
        "    random_state = trial.suggest_int('random_state', 0, 1000)\n",
        "    \n",
        "    model = GaussianMixture(\n",
        "        n_components=n_components,\n",
        "        covariance_type=covariance_type,\n",
        "        init_params=init_params,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        if len(set(labels)) <= 1:\n",
        "            return -1.0\n",
        "        return silhouette_score(X_weight, labels)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in GMM: {e}\")\n",
        "        return -1.0\n",
        "\n",
        "def objective_hdbscan(trial):\n",
        "    min_cluster_size = trial.suggest_int('min_cluster_size', 5, 50)\n",
        "    min_samples = trial.suggest_int('min_samples', 2, 10)\n",
        "    \n",
        "    model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
        "    \n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        unique_labels = set(labels)\n",
        "        if len(unique_labels) <= 1 or (len(unique_labels) == 2 and -1 in unique_labels):\n",
        "            return -1.0\n",
        "        \n",
        "        # Filter noise points jika diperlukan\n",
        "        mask = labels != -1\n",
        "        if np.sum(mask) <= 1:\n",
        "            return -1.0\n",
        "        \n",
        "        score = silhouette_score(X_weight[mask], labels[mask])\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in HDBSCAN: {e}\")\n",
        "        return -1.0\n",
        "\n",
        "def objective_dbscan(trial):\n",
        "    eps = trial.suggest_float('eps', 0.1, 2.0, log=True)\n",
        "    min_samples = trial.suggest_int('min_samples', 2, 10)\n",
        "    \n",
        "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    \n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        unique_labels = set(labels)\n",
        "        \n",
        "        # Check if useful clusters were formed\n",
        "        if len(unique_labels) <= 1 or (len(unique_labels) == 2 and -1 in unique_labels):\n",
        "            return -1.0\n",
        "            \n",
        "        # Handle noise points for silhouette score calculation\n",
        "        if -1 in unique_labels:\n",
        "            # Filter out noise points\n",
        "            mask = labels != -1\n",
        "            if sum(mask) <= 1:  # Not enough non-noise points\n",
        "                return -1.0\n",
        "            filtered_data = X_weight[mask]\n",
        "            filtered_labels = labels[mask]\n",
        "            score = silhouette_score(filtered_data, filtered_labels)\n",
        "        else:\n",
        "            score = silhouette_score(X_weight, labels)\n",
        "            \n",
        "        # Adjust score based on ratio of noise points\n",
        "        if -1 in unique_labels:\n",
        "            noise_ratio = np.sum(labels == -1) / len(labels)\n",
        "            if noise_ratio > 0.5:  # If more than 50% points are noise\n",
        "                score *= (1 - noise_ratio)  # Penalize for excessive noise\n",
        "                \n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in DBSCAN: {e}\")\n",
        "        return -1.0\n",
        "\n",
        "def objective_optics(trial):\n",
        "    min_samples = trial.suggest_int('min_samples', 2, 10)\n",
        "    xi = trial.suggest_float('xi', 0.01, 0.3)\n",
        "    min_cluster_size = trial.suggest_float('min_cluster_size', 0.05, 0.2)\n",
        "    cluster_method = trial.suggest_categorical('cluster_method', ['xi', 'dbscan'])\n",
        "    \n",
        "    model = OPTICS(\n",
        "        min_samples=min_samples,\n",
        "        xi=xi,\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        cluster_method=cluster_method\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        unique_labels = set(labels)\n",
        "        \n",
        "        # Check if useful clusters were formed\n",
        "        if len(unique_labels) <= 1 or (len(unique_labels) == 2 and -1 in unique_labels):\n",
        "            return -1.0\n",
        "            \n",
        "        # Handle noise points for silhouette score calculation\n",
        "        if -1 in unique_labels:\n",
        "            # Filter out noise points\n",
        "            mask = labels != -1\n",
        "            if sum(mask) <= 1:  # Not enough non-noise points\n",
        "                return -1.0\n",
        "            filtered_data = X_weight[mask]\n",
        "            filtered_labels = labels[mask]\n",
        "            score = silhouette_score(filtered_data, filtered_labels)\n",
        "        else:\n",
        "            score = silhouette_score(X_weight, labels)\n",
        "            \n",
        "        # Adjust score based on ratio of noise points\n",
        "        if -1 in unique_labels:\n",
        "            noise_ratio = np.sum(labels == -1) / len(labels)\n",
        "            if noise_ratio > 0.5:  # If more than 50% points are noise\n",
        "                score *= (1 - noise_ratio)  # Penalize for excessive noise\n",
        "                \n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in OPTICS: {e}\")\n",
        "        return -1.0\n",
        "\n",
        "def objective_meanshift(trial):\n",
        "    bandwidth = trial.suggest_float('bandwidth', 0.3, 2.0)\n",
        "    bin_seeding = trial.suggest_categorical('bin_seeding', [True, False])\n",
        "    cluster_all = trial.suggest_categorical('cluster_all', [True, False])\n",
        "    \n",
        "    model = MeanShift(\n",
        "        bandwidth=bandwidth,\n",
        "        bin_seeding=bin_seeding,\n",
        "        cluster_all=cluster_all\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        n_clusters = len(set(labels))\n",
        "        \n",
        "        if n_clusters <= 1:\n",
        "            return -1.0\n",
        "        if n_clusters >= X_weight.shape[0] - 1:  # Too many clusters\n",
        "            return -1.0\n",
        "            \n",
        "        score = silhouette_score(X_weight, labels)\n",
        "        \n",
        "        # Slightly penalize for too many clusters\n",
        "        if n_clusters > 10:\n",
        "            score *= (1 - (n_clusters - 10) * 0.01)\n",
        "            \n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in MeanShift: {e}\")\n",
        "        return -1.0\n",
        "\n",
        "def objective_agglomerative(trial):\n",
        "    n_clusters = trial.suggest_int('n_clusters', 2, 10)\n",
        "    linkage = trial.suggest_categorical('linkage', ['ward', 'complete', 'average', 'single'])\n",
        "    metric = trial.suggest_categorical('metric', ['euclidean', 'l1', 'l2', 'manhattan'])\n",
        "    \n",
        "    if linkage == 'ward' and metric != 'euclidean':\n",
        "        metric = 'euclidean'\n",
        "        \n",
        "    \n",
        "    model = AgglomerativeClustering(\n",
        "        n_clusters=n_clusters, \n",
        "        linkage=linkage,\n",
        "        metric=metric\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        labels = model.fit_predict(X_weight)\n",
        "        if len(set(labels)) <= 1:\n",
        "            return -1.0\n",
        "        score = silhouette_score(X_weight, labels)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in AgglomerativeClustering: {e}\")\n",
        "        return -1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optuna Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "algorithms = {\n",
        "    'KMeans': objective_kmeans,\n",
        "    'BisectingKMeans': objective_bisecting_kmeans,\n",
        "    'GaussianMixture': objective_gmm,\n",
        "    # 'HDBSCAN': objective_hdbscan,\n",
        "    # 'DBSCAN': objective_dbscan,\n",
        "    # 'OPTICS': objective_optics,\n",
        "    # 'MeanShift': objective_meanshift,\n",
        "    # 'AgglomerativeClustering': objective_agglomerative,\n",
        "}\n",
        "\n",
        "n_trials = 200\n",
        "results = {}\n",
        "best_params = {}\n",
        "\n",
        "print(\"\\n--- Running Hyperparameter Optimization with Optuna ---\")\n",
        "for algo_name, objective in algorithms.items():\n",
        "    print(f\"\\nOptimizing {algo_name}...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    results[algo_name] = study.best_value\n",
        "    best_params[algo_name] = study.best_params\n",
        "    \n",
        "    print(f\"Best parameters for {algo_name}: {study.best_params}\")\n",
        "    print(f\"Best silhouette score: {study.best_value:.4f}\")\n",
        "\n",
        "best_models = {}\n",
        "evaluation_results = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_sse(X, labels, centroids):\n",
        "    sse = 0.0\n",
        "    for i in range(len(centroids)):\n",
        "        cluster_points = X[labels == i]\n",
        "        if len(cluster_points) == 0:\n",
        "            continue\n",
        "        cluster_points = np.array(cluster_points)\n",
        "        centroid = np.array(centroids[i]).reshape(1, -1)\n",
        "        sse += np.sum(np.square(cluster_points - centroid))\n",
        "    return float(sse) \n",
        "\n",
        "evaluation_results = {}\n",
        "best_models = {}\n",
        "\n",
        "for algo_name, params in best_params.items():\n",
        "    if algo_name == 'KMeans':\n",
        "        model = KMeans(**params)\n",
        "    elif algo_name == 'BisectingKMeans':\n",
        "        model = BisectingKMeans(**params)\n",
        "    elif algo_name == 'GaussianMixture':\n",
        "        model = GaussianMixture(**params)\n",
        "    elif algo_name == 'HDBSCAN':\n",
        "        model = HDBSCAN(**params)\n",
        "    elif algo_name == 'DBSCAN':\n",
        "        model = DBSCAN(**params)\n",
        "    elif algo_name == 'OPTICS':\n",
        "        model = OPTICS(**params)\n",
        "    elif algo_name == 'MeanShift':\n",
        "        model = MeanShift(**params)\n",
        "    elif algo_name == 'AgglomerativeClustering':\n",
        "        model = AgglomerativeClustering(**params)\n",
        "\n",
        "    best_models[algo_name] = model\n",
        "\n",
        "    try:\n",
        "        model.fit(X_weight)\n",
        "        \n",
        "        try:\n",
        "            labels = model.labels_\n",
        "        except:\n",
        "            labels = model.fit_predict(X_weight)\n",
        "\n",
        "        labels = np.array(labels)\n",
        "        unique_labels = set(labels)\n",
        "        n_clusters = len(unique_labels)\n",
        "\n",
        "        if -1 in unique_labels:\n",
        "            n_clusters -= 1\n",
        "            noise_ratio = np.sum(labels == -1) / len(labels)\n",
        "            non_noise_mask = labels != -1\n",
        "            X_eval = X_weight[non_noise_mask]\n",
        "            labels_eval = labels[non_noise_mask]\n",
        "        else:\n",
        "            noise_ratio = 0\n",
        "            X_eval = X_weight\n",
        "            labels_eval = labels\n",
        "\n",
        "        if len(set(labels_eval)) <= 1:\n",
        "            print(f\"{algo_name}: Invalid clustering (insufficient non-noise points)\")\n",
        "            evaluation_results[algo_name] = {\n",
        "                'silhouette': -1,\n",
        "                'calinski_harabasz': -1,\n",
        "                'davies_bouldin': -1,\n",
        "                'sse': -1,\n",
        "                'model': model,\n",
        "                'n_clusters': n_clusters,\n",
        "                'noise_ratio': noise_ratio\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        # Hitung metrik evaluasi\n",
        "        sil_score = silhouette_score(X_eval, labels_eval)\n",
        "        ch_score = calinski_harabasz_score(X_eval, labels_eval)\n",
        "        db_score = davies_bouldin_score(X_eval, labels_eval)\n",
        "\n",
        "        # SSE\n",
        "        if algo_name == 'GaussianMixture':\n",
        "            # Get predictions and means\n",
        "            labels = model.fit_predict(X_eval)\n",
        "            centroids = model.means_\n",
        "            \n",
        "            # Calculate metrics using numpy arrays\n",
        "            labels = np.array(labels)\n",
        "            X_eval_array = np.array(X_eval)\n",
        "            \n",
        "            # Calculate evaluation metrics\n",
        "            sil_score = silhouette_score(X_eval_array, labels)\n",
        "            ch_score = calinski_harabasz_score(X_eval_array, labels)\n",
        "            db_score = davies_bouldin_score(X_eval_array, labels)\n",
        "            \n",
        "            # Calculate SSE\n",
        "            sse = compute_sse(X_eval_array, labels, centroids)\n",
        "            \n",
        "            evaluation_results[algo_name] = {\n",
        "                'silhouette': float(sil_score),\n",
        "                'calinski_harabasz': float(ch_score),\n",
        "                'davies_bouldin': float(db_score),\n",
        "                'sse': float(sse),\n",
        "                'model': model,\n",
        "                'n_clusters': params['n_components'],\n",
        "                'noise_ratio': 0.0\n",
        "            }\n",
        "            \n",
        "            print(f\"{algo_name}\\nSilhouette={sil_score:.4f}, Calinski-Harabasz={ch_score:.1f}, Davies-Bouldin={db_score:.2f}, SSE={sse:.2f}, Clusters={n_clusters}\")\n",
        "                \n",
        "        else:\n",
        "            if hasattr(model, \"inertia_\"):\n",
        "                sse = model.inertia_\n",
        "            elif hasattr(model, \"cluster_centers_\"):\n",
        "                sse = compute_sse(X_eval, labels_eval, model.cluster_centers_)\n",
        "            else:\n",
        "                centroids = np.array([X_eval[labels_eval == i].mean(axis=0) for i in np.unique(labels_eval)])\n",
        "                sse = compute_sse(X_eval, labels_eval, centroids)\n",
        "\n",
        "            print(f\"{algo_name}\\nSilhouette={sil_score:.4f}, Calinski-Harabasz={ch_score:.1f}, Davies-Bouldin={db_score:.2f}, SSE={sse:.2f}, Clusters={n_clusters}, Noise={noise_ratio*100:.1f}%\\n\")\n",
        "\n",
        "            evaluation_results[algo_name] = {\n",
        "                'silhouette': sil_score,\n",
        "                'calinski_harabasz': ch_score,\n",
        "                'davies_bouldin': db_score,\n",
        "                'sse': sse,\n",
        "                'model': model,\n",
        "                'n_clusters': n_clusters,\n",
        "                'noise_ratio': noise_ratio\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{algo_name}: Error in evaluation - {str(e)}\")\n",
        "        evaluation_results[algo_name] = {\n",
        "            'silhouette': -1,\n",
        "            'calinski_harabasz': -1,\n",
        "            'davies_bouldin': -1,\n",
        "            'sse': -1,\n",
        "            'model': model,\n",
        "            'n_clusters': 0,\n",
        "            'noise_ratio': 0\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        'Algorithm': algo,\n",
        "        'Silhouette': metrics['silhouette'],\n",
        "        'Calinski-Harabasz': metrics['calinski_harabasz'],\n",
        "        'Davies-Bouldin': metrics['davies_bouldin'],\n",
        "        'SSE': metrics['sse'],\n",
        "        'Clusters': metrics['n_clusters'],\n",
        "        'Noise Ratio': metrics['noise_ratio'] * 100\n",
        "    }\n",
        "    for algo, metrics in evaluation_results.items()\n",
        "])\n",
        "\n",
        "metrics_to_plot = ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'SSE']\n",
        "n_metrics = len(metrics_to_plot)\n",
        "\n",
        "fig, axes = plt.subplots(1, n_metrics, figsize=(5 * n_metrics, 6), sharey=False)\n",
        "\n",
        "palette = sns.color_palette(\"husl\", len(results_df['Algorithm']))\n",
        "algo_colors = dict(zip(results_df['Algorithm'], palette))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    sns.barplot(\n",
        "        data=results_df,\n",
        "        x='Algorithm',\n",
        "        y=metric,\n",
        "        ax=ax,\n",
        "        palette=[algo_colors[algo] for algo in results_df['Algorithm']]\n",
        "    )\n",
        "    \n",
        "    ax.set_title(f'{metric} Score', fontsize=14)\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.suptitle('Clustering Evaluation Metrics by Algorithm', fontsize=16, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Clustering Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_models = {k: v for k, v in evaluation_results.items() if v['silhouette'] > 0}\n",
        "if valid_models:\n",
        "    best_algo = max(valid_models, key=lambda x: valid_models[x]['silhouette'])\n",
        "    best_score = valid_models[best_algo]['silhouette']\n",
        "    best_model = valid_models[best_algo]['model']\n",
        "    n_clusters = valid_models[best_algo]['n_clusters']\n",
        "    \n",
        "    print(f\"\\nBest clustering algorithm: {best_algo}\")\n",
        "    print(f\"Best parameters: {best_params[best_algo]}\")\n",
        "    print(f\"Silhouette score: {best_score:.4f}\")\n",
        "    print(f\"Number of clusters: {n_clusters}\")\n",
        "    \n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_weight)\n",
        "    \n",
        "    if best_algo == 'GaussianMixture':\n",
        "        cluster_labels = best_model.predict(X_weight)\n",
        "    else:\n",
        "        cluster_labels = best_model.labels_\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', s=50)\n",
        "    plt.title(f'Clusters by {best_algo} (PCA projection)')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.show()\n",
        "    \n",
        "    if best_algo in ['KMeans', 'BisectingKMeans']:\n",
        "        centroids = best_model.cluster_centers_\n",
        "        feature_importance = np.std(centroids, axis=0)\n",
        "        feature_importance = feature_importance / np.sum(feature_importance)\n",
        "\n",
        "        print(\"\\nFeature importance for clustering:\")\n",
        "        for i, feature in enumerate(features):\n",
        "            print(f\"{feature}: {feature_importance[i]:.4f}\")\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(features, feature_importance)\n",
        "        plt.title('Feature Importance for Clustering')\n",
        "        plt.xlabel('Features')\n",
        "        plt.ylabel('Importance')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"\\nNo valid clustering models found with positive silhouette scores.\")\n",
        "    print(\"Consider revisiting your feature engineering or preprocessing steps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save & Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Best Model\n",
        "if 'best_model' in locals() and best_score > 0:\n",
        "    with open('best_model.pkl', 'wb') as f:\n",
        "        pickle.dump(best_model, f)\n",
        "    print(\"Best model saved as 'best_model.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Saved Best Model\n",
        "with open('best_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# New Dataframe for prediction\n",
        "new_data = pd.DataFrame({\n",
        "    'sentences': [113], \n",
        "    'page': [13], \n",
        "    'timing': [30], \n",
        "    'plagiarism': [74.00]\n",
        "})\n",
        "\n",
        "X_new_scaled = scaler.transform(new_data)\n",
        "X_new_scaled = pd.DataFrame(X_new_scaled, columns=features)\n",
        "\n",
        "X_new_weight = X_new_scaled * weights\n",
        "X_new_weight = pd.DataFrame(X_new_weight, columns=features)\n",
        "\n",
        "print(X_new_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4JThfbit_8y",
        "outputId": "678e00b2-a47b-4ce2-b8ff-14e405ab7a3c"
      },
      "outputs": [],
      "source": [
        "# K-Means/GMM prediction\n",
        "prediction = loaded_model.predict(X_new_weight)\n",
        "print(\"Prediction:\", prediction)\n",
        "\n",
        "# HDBSCAN prediction\n",
        "# labels, strengths = hdbscan.prediction.approximate_predict(loaded_model, new_data)\n",
        "# print(\"Label:\", labels)\n",
        "# print(\"Cluster strengths:\", strengths)\n",
        "\n",
        "# All other models is not supported for prediction except KMeans, Bisecting KMeans, GMM, and HDBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Large Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langgraph.graph import StateGraph, END\n",
        "# from langchain.schema import SystemMessage, HumanMessage\n",
        "# from langchain_groq import ChatGroq\n",
        "\n",
        "# # Inisialisasi Groq LLM\n",
        "# llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
        "\n",
        "# # Node 1: input\n",
        "# def input_node(state):\n",
        "#     return state\n",
        "\n",
        "# # Node 2: jawab\n",
        "# def answer_node(state):\n",
        "#     user_question = state[\"question\"]\n",
        "#     response = llm([\n",
        "#         SystemMessage(content=\"Kamu adalah asisten AI.\"),\n",
        "#         HumanMessage(content=user_question)\n",
        "#     ])\n",
        "#     return {\"answer\": response.content}\n",
        "\n",
        "# # Bangun graph\n",
        "# graph_builder = StateGraph()\n",
        "\n",
        "# graph_builder.add_node(\"InputNode\", input_node)\n",
        "# graph_builder.add_node(\"AnswerNode\", answer_node)\n",
        "# graph_builder.set_entry_point(\"InputNode\")\n",
        "# graph_builder.add_edge(\"InputNode\", \"AnswerNode\")\n",
        "# graph_builder.add_edge(\"AnswerNode\", END)\n",
        "\n",
        "# # Compile graph\n",
        "# graph = graph_builder.compile()\n",
        "\n",
        "# # Generate gambar graph\n",
        "# graph.get_graph().draw(\"langgraph_simple.png\", format=\"png\", prog=\"dot\")\n",
        "\n",
        "# print(\"✅ Diagram graph telah disimpan sebagai langgraph_simple.png\")\n",
        "\n",
        "# # Jalankan graph (opsional)\n",
        "# state = {\"question\": \"Apa itu LangGraph?\"}\n",
        "# result = graph.invoke(state)\n",
        "# print(\"\\nJawaban:\")\n",
        "# print(result[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
